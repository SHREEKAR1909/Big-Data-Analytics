# -*- coding: utf-8 -*-
"""BDA Customer Churn Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y5WqPpebNGIuJGhrPCTQWWb6gGkz-JP7
"""

# Step 1: Install Necessary Libraries
!pip install pyspark
!pip install findspark

# Step 2: Import Libraries and Initialize SparkSession
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, count, avg, round, desc
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Initialize SparkSession
spark = SparkSession.builder.appName("CustomerChurnAnalysis").getOrCreate()

# Step 3: Load and Explore the Dataset
from google.colab import files
uploaded = files.upload()

data_path = '/content/telecom_churn.csv'
data = spark.read.csv(data_path, header=True, inferSchema=True)

data.printSchema()
data.show(5)

print("Checking for null values:")
data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns]).show()

duplicates_count = data.count() - data.distinct().count()
print(f"Number of duplicates: {duplicates_count}")

data = data.distinct()

print("Summary statistics:")
data.describe().show()

# Step 4: Data Transformation
data = data.withColumn(
    "Day Usage Category",
    when(col("Total day minutes") > 250, "High")
    .when(col("Total day minutes") > 150, "Medium")
    .otherwise("Low")
)

data = data.withColumn("Churn", when(col("Churn") == True, "Yes").otherwise("No"))

categorical_cols = ['State', 'International plan', 'Voice mail plan', 'Churn']
numerical_cols = ['Account length', 'Number vmail messages', 'Total day minutes',
                  'Total day calls', 'Total eve minutes', 'Total eve calls',
                  'Total night minutes', 'Total night calls', 'Total intl minutes',
                  'Total intl calls', 'Customer service calls']

indexers = [StringIndexer(inputCol=col, outputCol=f"{col}_index") for col in categorical_cols]

assembler = VectorAssembler(
    inputCols=numerical_cols + [f"{col}_index" for col in categorical_cols[:-1]],
    outputCol="features"
)

from pyspark.ml import Pipeline
pipeline = Pipeline(stages=indexers + [assembler])
data_transformed = pipeline.fit(data).transform(data)

final_data = data_transformed.select("features", col("Churn_index").alias("label"))

# Step 5: Model Building and Evaluation
train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)

rf = RandomForestClassifier(labelCol="label", featuresCol="features", maxBins=60) #  maxBins=60 splitting the data into ranges

paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 50]).build()
#to test different configurations of hyperparameters.

cv = CrossValidator(
    estimator=rf,
    estimatorParamMaps=paramGrid,
    evaluator=MulticlassClassificationEvaluator(labelCol="label", metricName="accuracy"),
    numFolds=2
)# to find best hyperparameters

cv_model = cv.fit(train_data)

predictions = cv_model.transform(test_data)
evaluator = MulticlassClassificationEvaluator(labelCol="label", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)

print(f"Test Accuracy: {accuracy:.4f}")

predictions.select("features", "label", "prediction").show()

# Step 6: Feature Importance
rf_model = cv_model.bestModel
feature_importances = rf_model.featureImportances.toArray()

# Map feature names to importance scores
features = numerical_cols + [f"{col}_index" for col in categorical_cols[:-1]]
feature_imp_df = spark.createDataFrame(
    [(features[i], float(feature_importances[i])) for i in range(len(features))],
    ["Feature", "Importance"]
)

print("Feature Importance:")
feature_imp_df.orderBy("Importance", ascending=False).show()

# Step 7: Additional Analysis
churn_distribution = data.groupBy("Churn").count()
churn_distribution.show()

service_calls_churn = data.groupBy("Customer service calls", "Churn").count().orderBy(desc("count"))
service_calls_churn.show()

day_minutes_churn = data.groupBy("Day Usage Category", "Churn").count().orderBy(desc("count"))
day_minutes_churn.show()

# Step 8: Save the Model
model_path = "/content/customer_churn_rf_model"
cv_model.bestModel.write().overwrite().save(model_path)
print(f"Model saved to {model_path}")

# Step 9: Visualizations
import matplotlib.pyplot as plt

# Convert feature importance to Pandas for plotting
importances = feature_imp_df.toPandas()
plt.figure(figsize=(10, 6))
plt.barh(importances["Feature"], importances["Importance"], color="skyblue")
plt.xlabel("Importance")
plt.ylabel("Features")
plt.title("Feature Importance")
plt.gca().invert_yaxis()
plt.show()

